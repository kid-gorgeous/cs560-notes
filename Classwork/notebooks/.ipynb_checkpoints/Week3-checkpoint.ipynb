{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c989b7",
   "metadata": {},
   "source": [
    "## Design and Analysis of Algorithms - Week 3 \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30043ca8",
   "metadata": {},
   "source": [
    "### Monday 1/30 - Analysis of the Insertion Sort\n",
    "\n",
    "### 2.2 Analyzing Algoritms\n",
    "\n",
    "***Analyzing*** an algorithm has come to mean predicting the resources that the algorithm requires. Occasionally, memory, bandwidth, or computer hardware are of primary concern, but most often it is computational time that must be measured. \n",
    "\n",
    "For each of these algorithms, the model of the implementation technology for resources of cost. It will be assumed that a one processor, random-access machine (RAM) model of computation as the implementation technology to understand each algorithm will be used as computer programs. On these machines, instructions are executed one after another, with no concurrent operations.\n",
    "\n",
    "The RAM model contains instructions commonly found in real computers: arithmetic (such as add, subtract, multiply, divide, remainder, floor, ceiling) data movement (load, store, copy), and control (conditional and unconditional branch, subroutine call and return). Each such instruction will take a constant amount of time. \n",
    "\n",
    "The RAM model data types are integer and floating point (for storing real numbers). The limit on the size of each word of data will be assumed. \n",
    "\n",
    "For example, when working with inputs of size *n*, we typically assume that integers are represented by *c* lg *n* bits for some constant $c \\geq 1$. $c \\geq 1$ is required to hold so that each word can hold the value of *n*, enabling the indexing of the individual input elements, and *c* is restricted to be a constant so the word size does not gorw to an arbitraily large size in constant linear time.\n",
    "\n",
    "*Real Computers* contain instructions not listed above, and such instructions represnet a gray area in the RAM model. For example, exponentiation is not a constant-time instruction. It takes several instructions to compute $x^{y}$ when *x* and *y* are real numbers. However, in some restricted situations, exponentiation is a constant-time operation. Computers have a \"shift left\" instruction, which in constant time shifts the bits of an integer by *k* positions to the left. In most cases, shifting the bits of an integer by one position to the left is equivalent to multiplaction by 2, so that shifting the bits by *k* postions to the left is equivalent to multiplicationby $2^{k}$. Therefore, such computers can compute $2^{2}$ in one constant-time instruction by shifting the integer 1 by *k* positions to the left, as long as *k* is no more than the number of bits in a computer word. For this predispositioned analysis, computations of $2^{k}$ as a constant-time operation when *k* is a small enough positive integer.\n",
    "\n",
    "**Do not** attempt to model the memory hierarchy that is common in contemporary computers. **Do not** model caches or virtual memory. \n",
    "\n",
    "Mathmatical tools required may include combinations, probability theory, algebraic dexterity, and the ablility to identify the most significant terms in a formula. The behaviors of an algorithm may be different for each possible input. It will allow for easy summarization in simple, easily understood formulas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803b00a",
   "metadata": {},
   "source": [
    "### Method of Analysis:\n",
    "\n",
    "The time taken by the *insertion sort* procedure depends on the input: sorting a thousand numbers may take longer than sorting three. Moreover, it can take different amounts of time to sort two input sequences of the same size (depending on if the have been sorted). \n",
    "\n",
    "In general, the time taken by an algorithm grows with the size of the input, so traditionally it will be describe the running time of a program as a function of the size of its input. \n",
    "\n",
    "The best notion for ***input size*** depends on the problem being studied. For many problems, such as sorting or computing discrete Fourier transforms, the most natural measure is the *number of items in the input* For example, the array size *n* for sorting. For other problems, such as multiplying two integers, the best measure of input size is the *total number of bits* needed to represent the input in ordinary binary notation. It is more appropriate to describe the size of the input with two numbers rather than one. If the input is a graph, then the input size van be described by the numbers of vertives and edges in the graph. \n",
    "\n",
    "The ***running time*** of an algorithm on a particular input is the number of primitive operations or \"steps\" excuted. It is convenient to dfine the notion of step so that it is as machine-independent as possible. A constant amount of time is required to execute each line of psuedocode. Each line may require a different amount of time, but it can be assumed that each execution of the *i*th line takes time $c_{i}$, where $c_{i}$ is a constant. It reflects how the pseudocode will be implemented on most actual computers. \n",
    "\n",
    "\n",
    "\n",
    "Throughout this discussion, expressions for running time will evolve from a messy formula that uses all the statement costs $c_{i}$ to a much simpler notation that is more concise and more easily manipulated. This simplier notation will also make it easy to determine whether one algorithm is more efficient than another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ed5da",
   "metadata": {},
   "source": [
    "## Analysis of the Insertion Sort:\n",
    "---\n",
    "\n",
    "**First**, present the Insertion Sort procedure with the time ***\"cost\"*** of each statement and the ***n*** number of times each statement is executed. For each *j* = 2, 3, ... *n*, where *n* = *A.length*, we let $t_{j}$ denote the number of times the **while** loop test in line 5 is executed for that value of *j*. When a **for** or **while** loop exists in the usual way (i.e., due to the test in the loop header), the test is executed one time more than than the loop body. \n",
    "\n",
    "<img src='./screenshots/insertion-sort-cost-time.png' width='300' height='300'>\n",
    "\n",
    "The running time of the algorithm is the sum of running times for each statement executed; a statement that takes $ c_{i}$ steps to execute and executes *n* times will contribute $ c_{i}n$ to the total running time. To compute the total running time $T(n)$, the running. time of the Insertion-Sort on an input of *n* values, we sum the products of the *cost* and *times* columns to obtain:\n",
    "\n",
    "$  T(n) = c_{1}n + c_{2}(n-1) + c_{3}(n-1) + c_{4}(n-1) + c_{5}\\Sigma_{j=2}^n t_{j} + c_{6}\\Sigma_{j=2}^n (t_{j}-1) + c_{7}\\Sigma_{j=2}^n (t_{j}-1) + c_{8}(n-1) $\n",
    "\n",
    "Even for inputs of a given-size, an algorithm's running time may depend on *which* input of that size is given. For example, in the Insertion-Sort, the best case occurs if the array is already sorrted. For each *j* = 2, 3, ..., *n*, it can be found that *A*[i] $\\leq$ *key* in line 5 when *i* has its initial value of *j* - 1. Thus $ t_{j} = 1 $ for *j* = 2, 3, ...,*n*, and the best-case running time is\n",
    "\n",
    "$ T(n) = c_{1}n + c{2}(n-1) + c_{4}(n-1) + c_{5}(n-1) + c_{8}(n-1)$\n",
    "$ = (c_{1} + c_{2} + c_{4} + c_{5} + c_{8})n - (c_{2} + c_{4} + c_{5} + c_{8}) $\n",
    "\n",
    "This running time can be expressed as $ an + b $ for *constants* *a* and *b* that depends on the statement costs $ c_{i} $, it is thus a ***linear function*** of *n*.\n",
    "\n",
    "If the array is in reverse sorted order- that is, in decreasing order- the worst case results, each element $ A[j] $ with each element in the intire sored subarray $ A[1 .. j-1] $ and so $ t_{j} = j $ for *j* = 2, 3, ..., *n*. Noting that:\n",
    "\n",
    "$ \\Sigma_{j=2}^n j = \\frac{n(n+1)}{2} - 1 $\n",
    "and\n",
    "$ \\Sigma_{j=2}^n (j-1) = \\frac{n(n-1)}{2} $\n",
    "\n",
    "In the worst case, the running time of the Insertion Sort is:\n",
    "\n",
    "$ \\scriptsize\n",
    "T(n) = c_{1}n + c_{2}(n-1) + c_{3}(n-1) + c_{4}(n-1) + c_{5}(\\frac{n(n+1)}{2} - 1) + c_{6}(\\frac{n(n-1)}{2}) + c_{7}(\\frac{n(n-1)}{2}) + c_{8}(n-1)\\\\ \n",
    "  \\scriptsize \n",
    "= (\\frac{c_{5}}{2} + \\frac{c_{6}}{2} + \\frac{c_{7}}{2})n^{2} + ( c_{1} + c_{2} + c_{4} + \\frac{c_{5}}{2} - \\frac{c_{6}}{2} - \\frac{c_{7}}{2} + c_{8}) - (c_{2} + c_{4} + c_{5} + c_{8}) $\n",
    "\n",
    "$c_{i}$ is the instruction steps to execute and will execute *n* times to contribute to the total running time. The case above expresses the worst-case running time as $ an^{2} + bn + c $ for constant *a, b,* and *c* that again depend on the statement costs $c_{i}$, it is thus a ***quadratic function*** of *n*.\n",
    "\n",
    "Typically, the insertion sort running time is fixed for a given input, but this behavior can very even for a fixed input. \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea19f748",
   "metadata": {},
   "source": [
    "### Worst-Case and Average-Case Analysis\n",
    "\n",
    "This book recommends finding both a best, and worst case time complexity. This shall usually concentrate on finding only the ***worst-case running time***, (the longest running time for *any* input of size *n*. \n",
    "\n",
    "Reasoning of Orientation:\n",
    "\n",
    "- The worst-case running time of an algorithm gives us an upper bound on the running time for any input. Knowing this provides a guarantee that the algorithm will never take any longer.\n",
    "- For some algorithms, the worst case occurs fairly often. The worst case should occur when the algorithm simply cannot execute. The absence of information may be frequent. \n",
    "- The \"average case\" is often roughly as bad as the worst case. \n",
    "    - Suppose that *n* numbers are choosen randomly and the insertion sort is randomly applied  \n",
    "         - How long will it take to determine where in subarray $ A[1 .. j-1]$ to insert element $ A[j] $\n",
    "             - On average, half the elements in $ A[1 .. j-1]$ are less than $ A[j] $, and half of the rest are greater. \n",
    "             - On average, checking half the subarray $ A[1 .. j-1] $, and so $ t_{j} $ is just about $ \\frac{j}{2} $.\n",
    "             - The resulting average-case running time turns out to be a quadratic function of the input size, similar to the worst-case running time.\n",
    "\n",
    "\n",
    "In some cases, the ***average-case*** running time of an algorithm will be of interest; a technique of ***probabilistic analysis*** applied to various algoritms. The scope of average-case analysis is limited, because it may not be apparent what constitutes an \"average\" input for a particular problem. Assuming all inputs of a given size are equally can be likely. In practice, this assumption may be violated, sometimes the use of a randomized algorithm will help make random choices, to allow a probabilistic analysis and yield an ***expected*** running time.  \n",
    "\n",
    "\n",
    "### Order of Growth\n",
    "\n",
    "Simplified abstraction help ease the analysis of the Insertion-Sort procedure. \n",
    "\n",
    "- First, ignore actual cost of each statement, where the constant $ c_{i} $ is considered their cost\n",
    "- Next, observe that these constants give us more detail than we really need: we expressed the worst-case running time as $ an^{2} + bn + c $ for some constants *a*,*b*, and *c* that depend on the statement costs $c_{i}$. Ingore the abstract costs $c_{i}$\n",
    "- Finally, $\\Theta$-notation will be used to define the ***rate-of-growth***, or the ***order of growth***\n",
    "\n",
    "The running time is very important. Consider only the leading term of a formula (e.g., $an^{2}$), since the lower-order terms are relatively insignificant for larfe values of *n*. Ingore the leading term's constant coefficient. Constant factors are less significant than the rate of growth in determining computational efficiency for large inputs. \n",
    "\n",
    "For the insertion sort, the lower-order terms are ignored and the leading term's constant coefficient, which leaves the factor of $n^{2}$ from the leading term. For the insertion sort, the *worst-case* running time of $\\Theta(n^{2})$ (theta of *n*-squared).\n",
    "\n",
    "Usually a algorithm is considered more efficient than another if its worst-case running time has a lower order of growth. Due to constant factors and lower-order terms, an algorithm whose running time has a higher order of growth might take less time for small inputs than an algorithm whose running time has a lower order of growth. But for large enough inputs, a $\\Theta(n^{2})$ algorithm, will run more quickly in the worst case than a $\\Theta(n^{2})$ algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6250e6f",
   "metadata": {},
   "source": [
    "## 2.3 Designing Algorithms\n",
    "\n",
    "There is a wide range of algorithm design techniques to choose from. The insertion sort uses an ***increamentral*** approach: having sorted the subarray *A*[1 .. *j* -1], we inserted the single element *A[j]* into its proper place, yielding the sorted subarray *A*[1 .. *j*].\n",
    "\n",
    "This section, an alternative design approach will be used, known as \"*divide-and-conquer*. It will be used to design a sorting algorithm whose worst-case running time is much less than that of insertion sort. One advantage of divide-and-conquer algorithms is that their rinning times are often easily determined using *divide*, *conquer*, and *combine*.\n",
    "\n",
    "### 2.3.1 The Divide-and-Conquer Approach\n",
    "\n",
    "This algorithm will utilize ***recursion*** in its structure: to solve a given problem, they call themselves relatively one or more times to deal with closely related subproblems. This algorithm will typically follow the approach of: breaking the problem into several subproblems that are similar to the original problem but smaller in size, it will solve the problem, and the combime the solutions to create a solution to the original problem. \n",
    "\n",
    "The divde-and-conquer paradigm involves three steps at each level of recursion: \n",
    "- **Divide** the problem into a number of subproblems that are smaller instances of the same problem.\n",
    "- **Conquer** the subproblems by solving them recursively. If the subproblem sizes are small enough, however, it will solve the subproblems in a straightforward manner.\n",
    "- **Combine** the solutions to the subproblems into the solution for the original problem\n",
    "\n",
    "#### The Merge Sort:\n",
    "\n",
    "The merge sort closely follows the divide-and-conquer pardigm. Intuitively it operates as follows:\n",
    "\n",
    "- **Divide**: Divide the *n*-element sequence to be sorted into two subsequences f *n*/2 elements each.\n",
    "- **Conquer**: Sort the two subsequences recursively using merge sort.\n",
    "- **Combine**: Merge the two sorted subsequences to produce the sorted answer.\n",
    "\n",
    "The recursion \"bottums out\" when the sequence to be sorted has length 1, in which case there is no work to be done, since every sequence of length 1 is already sorted. \n",
    "\n",
    "The key operation of the merge sort algorithm is the merging of two sorted sequence in the \"combine\" step. The merge will be made by calling an auxiliary procedure MERGE(*A, p, q, r*), where *A* is an array and *p,q,* and *r* are indeices into the array such that $p \\leq q \\leq r$. The procedure assumes that the subarrays *A*[p .. q] and *A*[q + 1 .. r] are in sorted order. It *merges* them to form a single sored subarray that replaces the current subarray *A*[p .. r].\n",
    "\n",
    "The MERGE procedure only takes time $\\Theta(n)$, where *n* = *r* - *p* + 1 is the total number of elements being merged, and it works as follows...\n",
    "\n",
    "Two sequences are divided up equally, with each sequence sorted from smallest to largest. (smallest ontop the stack). When we wish to merge the two sequences into a single sorted output pile, where each sequence is said to be unknown. The basic step of choosing the smaller of the two elements in each sequence, and placing it into the original list. This step will repeat until one of the sequences is empty. At which time the remaining input sequence is concatenated at the end of the original sequence. Computationally, each basic step takes constant ime, since we are comparing just the two current elements from each list. Since at most *n* basic steps are performed, merging takes $\\Theta(n)$ time.\n",
    "\n",
    "The following psuedocode implements the above idea, but with an additional twist that avoids havin to check whether either pile is empty in each basic step. At the end of each divided sequence is placed a ***sentinal*** character, which contains a special value that will simplify the code. The character $\\infty$ is used as the sentinal value, and whenever encountered, it identifies that nothing in the sequence can be smaller unless both sentinal values have been encountered. Once this happens, all the nonsentinel characters have already been placed onto the output pile. Since it is known that exactly $r - p + 1$ elements will be placed into the output sequence, and will cease once $r - p + 1$ steps have been performed.\n",
    "\n",
    "<img src='./screenshots/basic-merge-sort-steps.png'>\n",
    "\n",
    "In detail, the MERGE procedure works as follows. Line 1 computes the length $n_{1}$ of the subarray *A*[q+1 .. r]. Arrays *L* and *R* are created to represent the (\"left\" and \"right\"), of lengths $n_{1}$ + 1 and $n_{2}$ + 1, in line 3; the extra position in each array will hold the sentinal character. The **for** loop of lines 4-5 copies the subarray *A*[p .. q] into *L*[1 .. $n_[1}$], and the **for** loop of lines 6-7 copies the subarray *A*[q + 1 .. r] into *R*[1 .. $n_{2}$]. Lines 8-9 put the sentinals at the ends of the arrays *L* and *R*. Lines 10-17, perform the $r - p + 1$ basic steps by maintaining the following loop invariant:\n",
    "\n",
    "At the start of each iteration of the **for** loop of lines 12-17, the subarray *A*[p .. k - 1] contains the $k - p$ smallest elements of *L*[i] and *R*[j] are the smallest elements of their arrays hat have not been copied back into *A*\n",
    "\n",
    "<img src='./screenshots/merge-steps-1.png'>\n",
    "<img src='./screenshots/merge-steps-2.png'>\n",
    "\n",
    "It must be show that this loop invariant holds prior to the first iteration of the **for** loop of lines 12-17, that each iteration of the loop maintains the invariant, and tat the invariant provides a useful property to show correctness when the loop terminates.\n",
    "\n",
    "##### Loop Invariant:\n",
    "\n",
    "- **Initialization**: Prior to the first iteration of the loop, $k = p$, so that the subarray *A*[p .. k - 1] is empty. The empty subarray contains the $k - p = 0$ smallest elements of *L* and *R*. Since $i = j = 1$, both *L*[i] and *R*[j] are the smallest elemenst of their arrays that have not been copied back into *A*\n",
    "\n",
    "- **Maintenance**: To see that each iteration maintains the loop invariant, let us first suppose that *L*[i]$\\leq$*R*[j]. Then *L*[i] is the smallest element not yet copied back into *A*. Because *A*[p .. k - 1] contains the $k - p$ smallest elements, after line 14 copies *L*[i] into*A*[k], the subarray *A*[p .. k] will contain the $k - p + 1$ smllest elements. Incrementing *k* (in the **for** loop update) and *i* (in line 15) reestablishes the loop invariant for the next iteration. If instead *L*[i] > *R*[j], then in lines 16-17 perform the appropriate action to maintain the loop invariant.\n",
    "\n",
    "- **Termination**: At termination, $k = r + 1$, By loop invarian, the subarray *A*[p .. k-1], which is *A*[p .. r], contains the $k - p = r - p + 1$ smallest elements of *L*[1 .. $n_{1} + 1$] and *R*[1 .. $n_{2} + 1$], in sorted order. The array *L* and *R* together contain $n_{1} + n_{2} + 2 = r - p + 3$ elements. All but two largest have been copied back into *A*, and these two largest elements are the sentinals. \n",
    "\n",
    "Now that the lists have been sorted; use MERGE procedure runs in $\\Theta(n)$ time, where $n = r - p + 1$, observe that line 1-3 and 8-11 takes constant time, and the **for** loops of lines 4-7 take $\\Theta(n_{1} + n_{2}) = \\Theta(n)$ time, and there are *n* iterations of he **for** loop of lines 12-17, each of which take constant time.\n",
    "\n",
    "The MERGE procedure can be used as a subroutine in the merge sort algorithm. The procedure MERGE-SORT(*A, p, r*) sorts the elements in the subarray *A*[p .. r]. If $p \\geq r$, the subarray has at most one element and is therefore already sorted. Otherwise, the divide step simply computes an index *q* that partitions *A*[p .. r] into two subarrays: *A*[p .. q], containing $\\lceil n/2 \\rceil$ elements, and *A*[*q* + 1 .. r], containing $\\lfloor n/2 \\rfloor$ elements.\n",
    "\n",
    "<img src='./screenshots/the-merge-sort.png'>\n",
    "\n",
    "To sort the entire sequence *A* = $\\langle A[1],A[2], ..., A[n] \\rangle$, the inital call MERGE-SORT(*A*, 1, *A.length*)a, where *A.length = n*. The operation of this procedure is bottom-up when *n* is the a power of 2. The aalgortihm consists of merging pairs of sequences of length 2, merging pairs of 1-item sequences to form sorted sequences of length 2, merging pairs of sequences of length 2 to form sorted sequences of length 4, and so on, until two sequences of length *n*/2 are merged to form the final sorted sequence of length *n*. \n",
    "\n",
    "<img src='./screenshots/sorted-sequence.png'>\n",
    "\n",
    "\n",
    "### 2.3.2 Analyzing Divide-and-Conquer Algorithms:\n",
    "\n",
    "When an algorithm contains a recursive call to itself, it will often describe its running time by a ***recurrence equation*** or ***recurrence***, which describes the overall running time on a problem of size *n* in terms of the running time on smaller inputs. \n",
    "\n",
    "Mathmatical tools to solve the recurrence and provide the performance of the algorithm.\n",
    "\n",
    "A recurrence for the running time of a divide-and-conquer algorithm falls out from the three steps of the basic paradigm. As before, let *T(n)* be the running time on a problem of size *n*. If the problem size is small enough, say $n \\leq c$ for some constant *c*, the straightforward solution takes constant time, which we write as $\\Theta(1)$. Suppose that the division of the problem yields *a* subproblems, each of which is 1/*b* the size of the original. (For merge sort, both *a* and *b* are 2, but a$\\neq$b). It takes time *T(n/b)* to solve one subproblem of size *n/b*, and so it takes time *aT(n/b)* to solve *a* of them. If *D(n)* time to divide the problem into subproblems and *C(n)* time to combine the solutions to the subproblems into the solution to the original problem, the recurrence will be:\n",
    "\n",
    "<img src='./screenshots/recurrence-time.png'>\n",
    "\n",
    "#### Analysis of Merge Sort:\n",
    "\n",
    "Although the psuedocode for MERGE-SORT works correctly when the number of elements is not even, our recurrence-based analysis is simplified if we assume hat the original problem size is a power of 2. Each divide step then yields two subsequences of size exactly *n/2*. The reason as follows to set up the recurrence for *T(n)*, the worst-case running time of merge sort on *n* numbers. Merge sort on just one element takes constant time. When that is *n* > 1 elements, the running time will be broken down as follows: \n",
    "\n",
    "- **Divide**: The divide step just computes the middle of the subarray, which takes constant time. Thus, *D(n)* = $\\Theta(1)$.\n",
    "- **Conquer**: Recursively solve two subproblems, each of size *n/2*, which contributes *2T(n/2)* to the running time. \n",
    "- **Combine**: The MERGE procedure on an *n*-element subarray takes time $\\Theta(n)$, and so *C(n)* = $\\Theta(n)$.\n",
    "\n",
    "When adding the function *D(n)* aand *C(n)* for the merge sort analysis, the function $\\Theta(n)$ and a function that is $\\Theta(1)$ are added. The sum is a linear function of *n*, that is $\\Theta(n)$. Adding it to the *2T(n/2)* term from the \"conquer\" step gives the recurrence for the worst-case running time *T(n)* of merge sort: \n",
    "\n",
    "<img src='./screenshots/merge-recurrence-running-time.png'>\n",
    "\n",
    "There is a \"master theorem\", which can show that *T(n)*  is $\\Theta$(*n* lg *n*), where lg *n* stands for $log_{2}n$ The logarithm function grows more slowly thaan any linear function, for large enough inputs, merge sort, with its $\\Theta$(*n* lg *n*) running time, outperforms insertion sort, whose running time is $\\Theta(n^{2})$, in the worst case.\n",
    "\n",
    "The \"master theorem\" is not needed to intuitively understan why the solution to the recurrence is *T(n)* = $\\Theta$(*n* lg *n*). The recurrence can be rewritten as: \n",
    "\n",
    "<img src='./screenshots/merge-recurrence-running-time-rewritten.png'>\n",
    "\n",
    "Where the constant *c* represents the time required to solve problems of size 1 as well as the time per array element of the divide and combine steps.\n",
    "\n",
    "<img src='./screenshots/recurrance-tree.png'>\n",
    "\n",
    "The image above shows how we can solve recurrence. For convenience, assume that *n* is an exact power of 2. Part (a) of the figure shows *T(n)*, which we expand in part (b) into an equivalent tree representing the reccurrance. The *cn* term is the root (the cost incurred at the top level of recursion), and the two subtrees of the root are the two smaller recurrences *T(n/2)*. Part (c) shows this process carried one step further by expanding *T(n/2)*. The cost incurred at each of the two sub-nodes at the second level of recursion is *cn*/2. We constituent part as determined by the recurrence, until the problem sizes get down to 1, each with a cost of *c*. Part(d) shows the resulting ***recursion tree***.\n",
    "\n",
    "Next, add the costs across each level of the tree. The top level has total cost *cn*, the next level down has total cost *c*(n/2) + *c*(n/2)= *cn*, the level after that has a total cost *c*(n/4) + *c*(n/4) + *c*(n/4) + *c*(n/4) = *cn*, and so on. In general the level *i* below the top has $2^{i}$ nodes, each contributing a cost of *c*(*n*/$2^{i}$), so that the *i*th level below the top has total cost $2^{i}$ *c*(*n*/$2^{i}$) = *cn*. The bottom level has *n* nodes, each contributing a cost of *c*, for total cost of *cn*.\n",
    "\n",
    "The total number of levels of the recursion tree is lg *n* + 1, where *n* is the number of leaves, corresponding to the input size. An informal inductive arguement justifies the claim. The base case occurs when *n* = 1, in which case the tree has only one leve. Since lg 1 = 0, then lg *n* + 1 gives the correct number of levels. Now assume as an inductive hypothesis that the number of levels of recursion tree with $2^{i}$ leaaves is lg $2^{i}$ + 1 = \n",
    "*i* + 1 (for any value of *i*, so that lg $2^{i}$ = *i*) It is assumed that the input size is a power of 2, the next input size to consider is $2^{i+1}$. A tree with *n* = $2^{i+1}$ leaves has one more level than a tree with $2^{i}$  leaves, and so the total number of levels is (i+1) + 1 = lg $2^{i+1}$ + 1.\n",
    "\n",
    "To compute the total cost represented by the recurrence, simply add up the costs of all the levels. The recursion tree has lg *n* + 1 levels, each costing *cn*, for a total cost of *cn*(lg *n* + 1) = *cn* lg *n* + *cn*. Ignoring the low-order term and the constant *c* gives the desired result of $\\Theta(n lg n)$.\n",
    "\n",
    "--- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28c2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6df2d58e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Zoom Class Notes:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Wednesday - Chapter 3 \n",
    "\n",
    "#### Growth of Functions:\n",
    "\n",
    "The *order of growth of the running time* of an algorithm gives a simple characterization of the algorithm's efficiency and also allows us to compare the relative performance of alternative algorithms.  \n",
    "\n",
    "Once the input size *n* becomes large enough, even the merge sort, with its $\\Theta(n lg n)$ worst-case running time, will out perform the insertion sort with its running time of $\\Theta(n^{2})$. \n",
    "\n",
    "The extra percision is not usually worth the effort of computing it. For large enough inputs, the multiplicative constants and lower-order terms of an exact running time are often dominated by the effects of the input size. \n",
    "\n",
    "When looking at large enough input sizes to make the order of growth of the running time relevant, it is important to consider the ***asymptotic*** efficency of algorithms. That is, we are concered with how the running time of an algorithm increases with the size of the input *in the limit*, as the size of the input increases without bound. Usually, an algorithm that is asymptotically more efficient will be the best choise for all *but* very small imputs.\n",
    "\n",
    "##### 3.1 - Asymptotic Notation:\n",
    "\n",
    "The notation used to describee the asymptotic running time of an algorithm in terms of the functions domain are the set of *all* natural numbers $ N = {0,1,2,...} $. This describes the worst-case running-time function *T(n)*. The function might be extended to the domain of real numbers or, alternatively, restrict it to a subset of the natural numbers. \n",
    "\n",
    "#### Asymptotic Notation, Functions, and Running Times:\n",
    "\n",
    "Asymptotic notation primarily describes the running times of algorithms, but they actually apply to functions. For instances, the insertion sort's worst-case running time as $an^{2} + bn + c$, for some constants, *a*, *b*, and *c*. By writing $\\Theta(n^{2})$ as a running time, but abstractly represents the full *quadratic* equation: $an^{2} + bn + c$. Both which characterizes the worst running-time. \n",
    "\n",
    "Functions to which the asymptotic notation will usually characterize the running times of algorithms. But, it can apply to functions that characterize some other aspect of the algorithms (the amount of space they use), or even to function that have nothing whatsoever to do with algorithms. \n",
    "\n",
    "Often this notation is used to make a \"blanket statement\" that will cover all inputs, and not just the worst case. The asymptotic notations are well suited to characterizing running times no matter what the input. \n",
    "\n",
    "#### $\\Theta$-notation: \n",
    "\n",
    "The worst-case running time of the insertion sort is *T(n)* = $\\Theta(n^{2})$. This function can be defined as the following: the given function *g(n)*, we denote by $\\Theta$(*g(n)*) the set of functions\n",
    "- $\\Theta$(*g(n)*) = {*f(n)*: there exist positive constants $c_{1}, c_{2}$, and $n_{0}$ such that 0 $\\leq c_{1}g(n) \\leq f(n) \\leq c_{2}g(n)$ for all *n* $\\geq n_{0}$}\n",
    "\n",
    "A function *f(n)* belongs to the set $\\Theta(g(n))$ if there exist positive constant $c_{1}$ and $c_{2}$ such that it can be \"sandwiched\" between $c_{1}g(n)$ and $c_{2}g(n)$, for sufficiently large *n*. Because $\\Theta(g(n))$ is a set. It could be written \"*f(n)*$\\in \\Theta(g(n))$\" to indicate that *f(n)* is a member of $\\Theta(g(n))$. Instead, it will usually been written as \"*f(n)* = $\\Theta(g(n))$\" to express the same notion. \n",
    "\n",
    "\n",
    "<img src='./screenshots/asymptotic-tight-bounds.png' width='400' height='300'>\n",
    "\n",
    "The image above gives an intuitive picture of functions *f(n)* and *g(n)* , where *f(n)* = $\\Theta(g(n))$. For all values of *n* at and to the right of $n_{0}$, the value of *f(n)* lies at or above $c_{1}$*g(n)* and at or below $c_{2}$*g(n)*. In other words, for all *n* $\\geq n_{0}$, the function *f(n)* is equal to *g(n)* to within a constant factor. It can be said that *g(n)* is ***asymptotically tight bound*** for *f(n)*.\n",
    "\n",
    "The definiton of $\\Theta(g(n))$ requires that every member *f(n)* $\\in$ $\\Theta(g(n))$ be ***asymptotically nonnegative***, that is, that *f(n)* be nonnegative whenever *n* is sufficiently large. An ***asymptoically positive*** function is one that is positive for all sufficiently large *n*. The function g(n) must be asymptotically nonnegative, or else the set $\\Theta(g(n))$ is empty. Therefore assume that every function is used within $\\Theta$-notation is asymptotically nonnegative. \n",
    "\n",
    "In Chapter 2, an informal notion of $\\Theta$-notation that amounted to throwing away lower-order terms and ignoring the leading coefficient of the highest-order term. To justify this intuition; use the formal definition to show that $\\frac{1}{2}n^{2} - 3n = \\Theta(n^{2})$. To do this, constants $c_{1}$, $c_{2}$, and $n_{0}$ such that\n",
    "\n",
    "- $c_{1}n^{2} \\leq \\frac{1}{2}n^{2} - 3n \\leq c_{2}n^{2}$\n",
    "\n",
    "for all $n \\geq n_{0}$. Dividing by $n^{2}$ yields\n",
    "\n",
    "- $c_{1} \\leq \\frac{1}{2} - \\frac{3}{n} \\leq c_{2}$\n",
    "\n",
    "The right-hand inequality hold for any value of $n \\geq 1$ by choosing any constant $c_{2} \\geq \\frac{1}{2}$. Likewise, the LHS inequality will hold for any value of $n \\geq 7$ by choosing any constant $c_{1} \\leq \\frac{1}{14}$. Thus, by choosing $c_{1}$ = 1/14, $c_{2}$ = 1/2, and $n_{0}$ = 7, we can verify that $\\frac{1}{2}n^{2} - 3n = \\Theta(n^{2})$. Other choices can exist, but it is important that *some* exist. Note that all the constants depend on the function $\\frac{1}{2}n^{2} - 3n$; a different function belonging to $\\Theta(n^{2})$ would usually require different constants.\n",
    "\n",
    "The formal definition $n^{3}$ can be used to verify that $6n^{3} \\neq \\Theta(n^{2})$. Suppose for the purpose of contridiction that $c_{2}$ to a value that is slightly larger permits the inequalities in the definition of $\\Theta$-notation to be satified. The coefficient of the highest order term can likewise be ignored, since it only changes $c_{1}$ and $c_{2}$ by a constant facotr equal to the coefficient. \n",
    "\n",
    "\n",
    "Consider the quadratic function $f(n) = an^{2} + bn + c$, where a,b,and c are constants and a > 0. Removing the lower terms and ignoring the constant yields $f(n) = \\Theta(n^{2})$. \n",
    "\n",
    "#### *O*-notation: \n",
    "\n",
    "The notation asymptotically bounds a function from above and below. When we have only an ***asymmptotic upper bound***. For a given function *g(n),\n",
    "\n",
    "- *O*(g(n))* = {*f(n)*: there exist positive constants *c* and $n_{0}$ such that $0 \\leq f(n) \\leq cg(n)0$ for all $ n \\geq n_{0}$}.\n",
    "\n",
    "Big-O notation is used to give an upper bound "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d8aabe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc651e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
