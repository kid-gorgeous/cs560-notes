{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03f4ad7",
   "metadata": {},
   "source": [
    "## Test Review:\n",
    "--- \n",
    "\n",
    "#### Question 3:\n",
    "\n",
    "The following data chart contains the variable n and several functions of n. Please reason whether function f(n) = $\\Theta(*g(n)*)$, where g(n) = n\\*n. (using a chart)\n",
    "\n",
    "Find c1,c2,n0\n",
    "\n",
    "if n=60, can the upper bound be found\n",
    "\n",
    "to find c2 using 8($n^{2}$) and 16($n^{2}$) for upper, \n",
    "and 1,2,4($n^{2}$) as c1 for the lower bound\n",
    "\n",
    "make note of the size of the n0 chosen, when n0 is small the bounds are loose, and when the n0 is large, the bounds are small\n",
    "\n",
    "#### Question 4: But Review\n",
    "\n",
    "<img src='./screenshots/maxSubarray.png' width='300'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd233b",
   "metadata": {},
   "source": [
    "## Week 5 - Chapter 4\n",
    "---\n",
    "### 4.1 - Find Max Subarray\n",
    "\n",
    "### 4.2 - Strassen's algorithm for matrix multiplaction \n",
    "\n",
    "If you have seen matricies before, then you probably know how to multiply them. \n",
    "\n",
    "If *A* = ($a_{ij}$) and *B* = ($a_{ij}$) are square *n* $\\times$ *n* matrices, then in the product *C* = *A*$\\cdot$*B*, we define the entry $c_{ij}$, for *i*, *j* = 1,2,...,*n* by\n",
    "\n",
    "- $c_{ij}$ = $\\Sigma_{k=1}^{n}$ $a_{ik} \\cdot b_{kj}$    (4.8)\n",
    "\n",
    "We must compute $n^{2}$ matrix entries, and each is the sum of *n* values. The following procedure takes *n*$\\times$*n* matrices *A* and *B* and multiples them, returning their *n*$\\times$*n* product *C*. Assume that each matrix has an attribute *rows*, giving the number of rows in the matrix.\n",
    "\n",
    "``` \n",
    "SQUARE-MATRIX-MULTIPLY(A, B)\n",
    "1 n = A.rows\n",
    "2 let C be a new n × n matrix\n",
    "3 for i = 1 to n\n",
    "4   for j = 1 to n\n",
    "5     cij = 0\n",
    "6     for k = 1 to n\n",
    "7       cij = cij + aik · bkj\n",
    "8 return C\n",
    "```\n",
    "\n",
    "The SQUARE-MATRIX-MULTIPLY procedure works as follow. The **for** loop of lines 3-7 computes the entries of each row *i*, and within a given row *i*, the **for** loop of lines 4-7 computes each of the entries $c_{ij}$ for each column *j*. Line 5 intializes $c_{ij}$ to 0 as we start computing the sum given in equation, and each iteration of the **for** loop of the lines 6-7 adds in one more term of equation (4.8).\n",
    "\n",
    "Because each of the triply-nested **for** loops runs exactly *n* iterations, and each execution of lines 7 takes constant time, SQUARE-MATRIX-MULTIPLY procedure takes $\\Theta(n^{3})$ time.\n",
    "\n",
    "You might at first think that any matrix multiplication algorithm must take $\\Omega(n^{3})$ time, since the natural definition of matrix multiplication requires that many multiplications. You would be incorrect, however; the way to multiply the matrices in *o*($n^{3}$) time. In this section, the Strassen's recursive algorithm for multiplying $n \\times n$ matrices. With this assumption, each divide step, the divide $n \\times n$ matrices into four $n/2 \\times n/2$ matrices, and by assuming that *n* is an exact power of 2, it is guaranteed that as long as $n \\leq 2$, the dimension $n/2$ is an integer. \n",
    "\n",
    "Suppose that we partition each of *A*,*B*, and *C* into four $n/2 \\times n/2$ matrices.\n",
    "\n",
    "A = ([$A_{11}  A_{12}$],[$A_{21}  A_{22}$]),   (4.9)\n",
    "B = ([$B_{11}  B_{12}$],[$B_{21}  B_{22}$]),\n",
    "C = ([$C_{11}  C_{12}$],[$C_{21}  C_{22}$])\n",
    "\n",
    "so that rewite the equation *C* = $A \\cdot B$ as \n",
    "\n",
    "([$C_{11}  C_{12}$],[$C_{21}  C_{22}$]) = ([$A_{11}  A_{12}$],[$A_{21}  A_{22}$])$\\cdot$([$B_{11}  B_{12}$],[$B_{21}  B_{22}$])\n",
    "\n",
    "Equation (4.10) corresponds to the four equations\n",
    "\n",
    "- $C_{11} = A_{11} \\cdot B_{11} + A_{12} \\cdot B_{21}$,  \n",
    "- $C_{11} = A_{11} \\cdot B_{12} + A_{12} \\cdot B_{22}$\n",
    "- $C_{21} = A_{21} \\cdot B_{11} + A_{22} \\cdot B_{21}$\n",
    "- $C_{22} = A_{21} \\cdot B_{12} + A_{22} \\cdot B_{22}$\n",
    "\n",
    "Each of these four equations specifies two multiplacations of $n/2 \\times n/2$ matrices and the addition of their $n/2 \\times n/2$ products. The can be used to create a straight-forward, recursive, divide-and-conquer algorithm: \n",
    "\n",
    "```\n",
    "SQUARE-MATRIX-MULTIPLY-RECURSIVE(*A*,*B*):\n",
    "1 n = A.rows\n",
    "2 let *C* be a new $n \\times n$ matrix\n",
    "3 **if** *n* == 1\n",
    "4    $c_{11} = a_{11} \\cdot b_{11}$\n",
    "5 **else** partition *A*, *B*, and *C* as in equations (4.9)\n",
    "6    $C_{11}$ = SQUARE-MATRIX-MULTIPLY-RECURSIVE($A_{11}$, $B_{11}$) + SQUARE-MATRIX-MULTIPLY-RECURSIVE($A_{12}$, $B_{21}$)\n",
    "7    $C_{12}$ = SQUARE-MATRIX-MULTIPLY-RECURSIVE($A_{11}$, $B_{11}$) + SQUARE-MATRIX-MULTIPLY-RECURSIVE($A_{12}$, $B_{21}$)\n",
    "8    $C_{21}$ = SQUARE-MATRIX-MULTIPLY-RECURSIVE($A_{11}$, $B_{12}$) + SQUARE-MATRIX-MULTIPLY-RECURSIVE($A_{12}$, $B_{22}$)\n",
    "9    $C_{22}$ = SQUARE-MATRIX-MULTIPLY-RECURSIVE($A_{21}$, $B_{12}$) + SQUARE-MATRIX-MULTIPLY-RECURSIVE($A_{22}$, $B_{22}$)\n",
    "10 **return**\n",
    "\n",
    "```\n",
    "\n",
    "These procedure glosses over one subtle but important imlementation detail. To partition create 12 additional $n/2 \\times n/2$ matrices, we would spend $\\Theta(n^{2})$ time copying entries. The matrices can be partitioned \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bf451",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "### 4.3 - The substitution method for solving recurrences \n",
    "\n",
    "\n",
    "\n",
    "### 4.4 - The recursion-tree method for solving recurrences\n",
    "\n",
    "\n",
    "\n",
    "### 4.5 - The master method for solving recurrences\n",
    "\n",
    "The master method provides a \"cookbook\" method for solving recurrences of the form:\n",
    "- $ T(n) = aT(n/b) + f(n)$ \n",
    "\n",
    "where $a \\geq 1$ and *b* > 1 are constants and *f(n)* is an asymptotcially positive function. To use the *master method*, you will need to memorize three cases, but then you will be able to solve many recurrences quite easily, often without pencil and paper.\n",
    "\n",
    "The recurrence describes the running time of an algorithm that divides a problem os size *n* into *a* subproblems, each of size *n/b*, where *a* and *b* are positive constants. The *a* subproblems are solved recusively, each in time $T(n/b)$. The function *f(n)* encompasses the cost of dividing the problem and combining the results of the subproblems. For example, the recurrence arising from Strassen's algorithm has *a* = 7, *b* = 2, and *f(n)* = $\\Theta(n^{2})$.\n",
    "\n",
    "As a matter of technical correctness, the recurrence is not actuallly well defined, because *n/b* might not be an integer. Replacing each of the *a* terms $T(n/b)$ with either $T(\\lfloor n/b \\rfloor)$ or $T(\\lceil n/b \\rceil)$ will not affect the asymptotic behavior of the recurrence, however it is convenient to omit the floor and ceiling functions when writing divide-and-conquer recurrences of this form. \n",
    "\n",
    "#### The master theorem:\n",
    "\n",
    "The master method depends on the following theorem.\n",
    "\n",
    "***Theorem 4.1 (Master theorem)***:\n",
    "Let $a \\geq 1$ and $b > 1$ be constants, let *f(n) be an asymptotically positive function, and let *T(n)* be defined on the nonnegative integers by the recurrence:\n",
    "\n",
    "- $T(n) = aT(n/b) + f(n)$,\n",
    "\n",
    "where we interpret *n/b* to mean either $\\lfloor n/b \\rfloor$ or $\\lceil n/b \\rceil$. The *T(n)* has the following asymptotic bounds:\n",
    "\n",
    "1. **If** $f(n) = O(n^{log_{b}a-\\varepsilon})$ for some constant $\\varepsilon > 0$, then $T(n) = \\Theta(n^{log_{b}}a)$, if you can find upper\n",
    "\n",
    "2. **If** $f(n) = \\Theta(n^{log_{b}}a)$, then $ T(n) = \\Theta(n^{log_{b}a} \\cdot lg n) $ , if you can find the boundaries\n",
    "\n",
    "3. **If** $f(n) = \\Omega(n^{log_{b}a+\\varepsilon})$ for some constant $\\varepsilon > 0$, and if $af(n/b) \\leq cf(n)$ for some constant *c* < 1 and all sufficently large *n*, then $T(n) = \\Theta(f(n))$, if you can find lower\n",
    "\n",
    "For each three cases: the function $n^{log_{b}a}$ will be compared to *f*(n). The larger of the two functions determines the solution to the recurrence. If, as in case 1, function $n^{log_{b}a}$ is the larger, than the solution is $T(n) = \\Theta($n^{log_{b}a}$ \\cdot lg n) = \\Theta(f(n)\\cdot lg n)$\n",
    "\n",
    "Beyond this intuition; the first case not only must *f*(n) be smaller than $n^{log_{b}a}$, is must be *polynomial* smaller. *f*(n) must be asymptotically *smaller* than $n^{log_{b}a}$ by a factor of $n^{\\varepsilon}$ for some constant ${\\varepsilon}$ > 0. In the third case, not only must *f*(n) be *larger* than $n^{log_{b}a}$, it also must be polynomially larger and in addition satisfy the \"regularity\" condition that $af(n/b) \\leq cf(n)$. This condition is satisfied by most of the polynomially bounded functions that we shall encounter. \n",
    "\n",
    "Note, the three cases do not cover all the possibilities for *f*(n).\n",
    "There is a gap between cases 1 and 3 when *f*(n) is smaller than $n^{log_{b}a}$ but not polynomially smaller. Similarly, there is a gap between cases 2 and 3 when *f*(n) is larger than $n^{log_{b}a}$ but not polynomially larger. If the function *f*(n) falls into one of these gaps, or if the regularity condition in case 3 fails to hold, you cannot use the master method to solve the recurrence.\n",
    "\n",
    "#### Using the master method:\n",
    "\n",
    "To use the master method, simply determine which case (if any) of the theorem applies and write down the answer.\n",
    "\n",
    "Ex 1) \n",
    "\n",
    "- $T(n) = 9T(2n/3) + 1$\n",
    "\n",
    "For this recurrence, we have *a* = 9, *b* = 3, *f*(n) = n, and thus we have that $n^{log_{b}a}$=$n^{log_{3}9}$ = $\\Theta(n^{2})$. Since $f(n) = \\Theta(n^{log_{3}9}- \\varepsilon)$, where $\\varepsilon$ = 1, case 1 can be applied from the *master* theorem and conclude that the solution is $T(n) = \\Theta(n^{2})$.\n",
    "\n",
    "Now consider,\n",
    "\n",
    "- $T(n) = 9T(2n/3) + 1$\n",
    "\n",
    "in which *a* = 1, *b* = 3/2, *f*(n) = 1, and $n^{log_{b}a}$ = $n^{log_{3/2}1}$ = $n^{0}$ = 1. Case applied, since $f(n) = \\Theta(n^{log_{b}a})$ = $\\Theta(1)$, and thus the solution to the recureences is T(*n*) = $\\Theta$(lg *n*).\n",
    "\n",
    "For the recurrence\n",
    "- $T(n) = 3T(n/4) + n lg n$\n",
    "\n",
    "in which $a = 3, b = 4, f(n) = n lg n$, and $n^{log_{b}a}$ = $n^{log_{4}3}$ = $O(n^{0.793})$. Since $f(n) = \\Omega(n^{log_{4}3+ \\varepsilon}$, where $\\varepsilon$ = 0.2, case 3 applies if we can show that the regularity condition holds for $f(n)$. For sufficiently large *n*, the recurrence $af(n/b) = 3(n/4) lg(n/4) \\leq (3/4)n lg n = cf(n)$ for c = 3/4. Consequently, by case 3, the solution to the recurrence is $T(n) = \\Theta(n lg n)$.\n",
    "\n",
    "\n",
    "*Notice*: The master method does not apply to the recurrence. \n",
    "\n",
    "$T(n) = 2T(n/2) + \\Theta(n)$\n",
    "\n",
    "characterizes the running times of the divide-and-conquer algorithm for both the maximun-subarray problem and merge sort..\n",
    "(As the prime example, omit starting the base vcase in the recurrence). Here, $ a = 2, b = 2, f(n) = \\Theta(n)$, and thus we have that $n^{log_{b}a}$ = $n^{log_{2}2}$ = $n$. Case 2 applies, since $f(n) = \\Theta(n)$, and so we have the solution $T(n) = \\Theta(n lg n)$.\n",
    "\n",
    "Consider the recurrence:\n",
    "\n",
    "- $T(n) = 8T(n/2) + \\Theta(n^{2})$\n",
    "\n",
    "describes the running time of the first divide-and-conquer algorithm that we saw for matrix multiplication. Now, $a = 8, b = 2, and f(n) = \\Theta(n^{2})$, and so $n^{log_{b}a}$ = $n^{log_{2}8}$ = $n^{3}$. Since $n^{3}$. Since $n^{3}$ is polynomially larger than $f(n)$(this means $f(n) = O(n^{3 - \\varepsilon})$ for $\\varepsilon = 1$), case 1 applies, and $T(n) = \\Theta(n^{3})$. \n",
    "\n",
    "Finally, consider recurrence\n",
    "\n",
    "$T(n) = 7T(n/2) + \\Theta(n^{2})$,\n",
    "\n",
    "which describes the running time of Strassen's algorthim. Here, $a = 7, b = 2, f(n) = \\Theta(n^{2})$. and thus $n^{log_{b}a}$ = $n^{log_{2}7}$. Rewriting $log_{2}7$ as lg 7 and recalling that 2.80 < lg 7 < 2.81, it can be seen that $f(n) = O(n^{lg 7 - \\varepsilon})$ for $\\varepsilon$ 0.8. Again, case 1 applies, and we have the solution $T(n) = \\Theta(n^{lg 7})$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Improving Understanding:\n",
    "It will improve *all* layers of the tree, to understand this better take into account the cost and the relation with the tree bound\n",
    "\n",
    "Defn: *Structure cost*d\n",
    "\n",
    "#### Using the master method:\n",
    "\n",
    "To use the master method \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 4.6 - Proof of the master theorem\n",
    "\n",
    "#### 4.6.1 - The proof for exact powers\n",
    "#### 4.6.2 - Floors and ceilings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53fdd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
